{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "\n",
    "class PETA_Dataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = {}\n",
    "        valid_extensions = {'.bmp', '.jpg', '.jpeg', '.png'}  # 支持的图片格式\n",
    "\n",
    "        # 遍历所有subset文件夹，读取标签文件\n",
    "        for subset in os.listdir(root_dir):\n",
    "            subset_path = os.path.join(root_dir, subset, 'archive')\n",
    "            if not os.path.isdir(subset_path):\n",
    "                continue\n",
    "\n",
    "            label_file = os.path.join(subset_path, 'Label.txt')\n",
    "\n",
    "            if not os.path.exists(label_file):\n",
    "                continue\n",
    "\n",
    "            # 读取标签文件，并保存每个ID的标签信息\n",
    "            with open(label_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    image_id = parts[0]\n",
    "                    tags = parts[1:]\n",
    "\n",
    "                    # 保存subset与ID组合后的唯一标识和personalMale标签的对应关系\n",
    "                    unique_id = f\"{subset}_{image_id}\"\n",
    "                    label = 1 if 'personalMale' in tags else 0\n",
    "                    self.labels[unique_id] = label\n",
    "\n",
    "            # 获取所有图片路径，支持多种图片格式\n",
    "            for img_file in os.listdir(subset_path):\n",
    "                ext = os.path.splitext(img_file)[-1].lower()\n",
    "                if ext in valid_extensions:\n",
    "                    # 获取文件名前的ID，即第一个下划线之前的部分\n",
    "                    image_id = img_file.split('_')[0]\n",
    "\n",
    "                    # 生成唯一ID，确保区分不同subset中的相同ID\n",
    "                    unique_id = f\"{subset}_{image_id}\"\n",
    "\n",
    "                    # 如果ID存在于标签文件中，则保存该图片路径和对应标签\n",
    "                    if unique_id in self.labels:\n",
    "                        img_path = os.path.join(subset_path, img_file)\n",
    "                        self.image_paths.append((img_path, self.labels[unique_id]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取图片路径和对应的标签\n",
    "        img_path, label = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Transform = transforms.Compose([transforms.Resize(size=(224, 224)), transforms.ToTensor()])\n",
    "Full_Dataset = PETA_Dataset(root_dir='./PETA dataset', transform=Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Num_Lables = 2\n",
    "Pre_Trained_Model = 'google/vit-base-patch16-224-in21k'\n",
    "ViT_Feature_Extractor = 'google/vit-base-patch16-224-in21k'\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-5\n",
    "Num_Workers = 0\n",
    "SHUFFLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples:  15200\n",
      "Number of test samples:  3800\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "train_size = int(0.8 * len(Full_Dataset))\n",
    "test_size = len(Full_Dataset) - train_size\n",
    "\n",
    "\n",
    "train_ds, test_ds = torch.utils.data.random_split(Full_Dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Number of train samples: \", len(train_ds))\n",
    "print(\"Number of test samples: \", len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=SHUFFLE,  num_workers=Num_Workers, drop_last=True)\n",
    "test_loader  = data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=Num_Workers, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, num_labels=Num_Lables):\n",
    "        super(ViTForImageClassification, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained(Pre_Trained_Model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        output = self.dropout(outputs.last_hidden_state[:,0])\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "#         if labels is not None:\n",
    "#           loss_fct = nn.CrossEntropyLoss()\n",
    "#           loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        if loss is not None:\n",
    "          return logits, loss.item()\n",
    "        else:\n",
    "          return logits, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.8/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "model = ViTForImageClassification(Num_Lables)\n",
    "# Feature Extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(ViT_Feature_Extractor, do_rescale=False)\n",
    "# Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# Cross Entropy Loss\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model= nn.DataParallel(model)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f7e94b30c7e4c7fa52920ec92518d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | train loss: 0.7130 | test accuracy: 0.62\n",
      "Epoch:  0 | train loss: 0.4439 | test accuracy: 0.84\n",
      "Epoch:  0 | train loss: 0.3076 | test accuracy: 0.88\n",
      "Epoch:  0 | train loss: 0.2118 | test accuracy: 0.94\n",
      "Epoch:  0 | train loss: 0.1811 | test accuracy: 0.94\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94bb97256c445fb9f229d4023d54ad1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  1 | train loss: 0.1985 | test accuracy: 0.92\n",
      "Epoch:  1 | train loss: 0.0989 | test accuracy: 0.89\n",
      "Epoch:  1 | train loss: 0.1512 | test accuracy: 0.91\n",
      "Epoch:  1 | train loss: 0.1815 | test accuracy: 1.00\n",
      "Epoch:  1 | train loss: 0.1584 | test accuracy: 0.95\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d09594650ce45269a7cee7aedda0ed6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  2 | train loss: 0.1484 | test accuracy: 0.94\n",
      "Epoch:  2 | train loss: 0.0771 | test accuracy: 0.92\n",
      "Epoch:  2 | train loss: 0.0491 | test accuracy: 0.94\n",
      "Epoch:  2 | train loss: 0.0515 | test accuracy: 0.97\n",
      "Epoch:  2 | train loss: 0.0207 | test accuracy: 0.91\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98c643891f044c2afb20580bc88f0af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  3 | train loss: 0.0150 | test accuracy: 0.89\n",
      "Epoch:  3 | train loss: 0.0184 | test accuracy: 0.94\n",
      "Epoch:  3 | train loss: 0.0170 | test accuracy: 0.91\n",
      "Epoch:  3 | train loss: 0.0215 | test accuracy: 0.89\n",
      "Epoch:  3 | train loss: 0.0581 | test accuracy: 0.94\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50b5b6c0d20c4d4bb79d09d844de5937",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  4 | train loss: 0.0113 | test accuracy: 0.98\n",
      "Epoch:  4 | train loss: 0.0219 | test accuracy: 0.95\n",
      "Epoch:  4 | train loss: 0.0706 | test accuracy: 0.95\n",
      "Epoch:  4 | train loss: 0.0151 | test accuracy: 0.98\n",
      "Epoch:  4 | train loss: 0.0295 | test accuracy: 0.98\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19e56d52e424e4796560ec49e86799b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  5 | train loss: 0.0199 | test accuracy: 0.97\n",
      "Epoch:  5 | train loss: 0.0069 | test accuracy: 0.95\n",
      "Epoch:  5 | train loss: 0.0051 | test accuracy: 0.91\n",
      "Epoch:  5 | train loss: 0.0187 | test accuracy: 0.91\n",
      "Epoch:  5 | train loss: 0.0081 | test accuracy: 0.94\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4acdbd924f4da7b2e2beb97b83e145",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  6 | train loss: 0.0047 | test accuracy: 0.98\n",
      "Epoch:  6 | train loss: 0.0043 | test accuracy: 0.94\n",
      "Epoch:  6 | train loss: 0.0179 | test accuracy: 0.89\n",
      "Epoch:  6 | train loss: 0.0788 | test accuracy: 0.94\n",
      "Epoch:  6 | train loss: 0.0061 | test accuracy: 0.92\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6444462c942348019faac19f25243d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  7 | train loss: 0.0033 | test accuracy: 0.95\n",
      "Epoch:  7 | train loss: 0.0045 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0362 | test accuracy: 0.97\n",
      "Epoch:  7 | train loss: 0.0031 | test accuracy: 0.95\n",
      "Epoch:  7 | train loss: 0.0043 | test accuracy: 0.86\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749e00b3daa74b51ae7edb4af32e9e51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  8 | train loss: 0.0939 | test accuracy: 0.94\n",
      "Epoch:  8 | train loss: 0.0032 | test accuracy: 0.97\n",
      "Epoch:  8 | train loss: 0.0040 | test accuracy: 0.98\n",
      "Epoch:  8 | train loss: 0.0427 | test accuracy: 0.95\n",
      "Epoch:  8 | train loss: 0.0053 | test accuracy: 0.98\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a71d9bd29ef41c6ad90793a3c43b33a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  9 | train loss: 0.0027 | test accuracy: 0.91\n",
      "Epoch:  9 | train loss: 0.0066 | test accuracy: 0.95\n",
      "Epoch:  9 | train loss: 0.0023 | test accuracy: 0.94\n",
      "Epoch:  9 | train loss: 0.0022 | test accuracy: 0.97\n",
      "Epoch:  9 | train loss: 0.0057 | test accuracy: 0.94\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# Train the model\n",
    "acc = []\n",
    "lss = []\n",
    "ep = []\n",
    "step = 1\n",
    "for epoch in range(EPOCHS):\n",
    "    for step, (x, y) in (p:=tqdm(enumerate(train_loader))):\n",
    "        p.set_description(f'Epoch {epoch}: {step}/{len(train_loader)}')\n",
    "        # Change input array into list with each batch being one element\n",
    "        x = np.array_split(np.squeeze(np.array(x)), BATCH_SIZE)\n",
    "        # Remove unecessary dimension\n",
    "        for index, array in enumerate(x):\n",
    "            x[index] = np.squeeze(array)\n",
    "        # Apply feature extractor, stack back into 1 tensor and then convert to tensor\n",
    "\n",
    "        x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
    "        # Send to GPU if available\n",
    "        x, y  = x.to(device), y.to(device)\n",
    "        b_x = Variable(x)   # batch x (image)\n",
    "        b_y = Variable(y)   # batch y (target)\n",
    "        # Feed through model\n",
    "        output, loss_train = model(b_x, None)\n",
    "        # Calculate loss\n",
    "        if loss_train is None:\n",
    "            loss_train = loss_func(output, b_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "          # train_losses.append(loss_train.item())\n",
    "        if step % 50 == 0:\n",
    "            # Get the next batch for testing purposes\n",
    "            test = next(iter(test_loader))\n",
    "            test_x = test[0]\n",
    "            # Reshape and get feature matrices as needed\n",
    "\n",
    "            test_x = np.array_split(np.squeeze(np.array(test_x)), BATCH_SIZE)\n",
    "            for index, array in enumerate(test_x):\n",
    "                test_x[index] = np.squeeze(array)\n",
    "            test_x = torch.tensor(np.stack(feature_extractor(test_x)['pixel_values'], axis=0))\n",
    "            # Send to appropirate computing device\n",
    "            test_x = test_x.to(device)\n",
    "            test_y = test[1].to(device)\n",
    "            # Get output (+ respective class) and compare to target\n",
    "            test_output, loss_test = model(test_x, test_y)\n",
    "            # val_losses.append(loss_test.item())\n",
    "            test_output = test_output.argmax(1)\n",
    "            # Calculate Accuracy\n",
    "            accuracy = (test_output == test_y).sum().item() / BATCH_SIZE\n",
    "            # accuracy_all.append(accuracy)\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss_train, '| test accuracy: %.2f' % accuracy)\n",
    "            acc.append(accuracy)\n",
    "            ep.append(step)\n",
    "            step = step + 1\n",
    "            lss.append(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PGC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
