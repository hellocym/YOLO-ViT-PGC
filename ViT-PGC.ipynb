{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libnccl.so.2: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/PGC/lib/python3.8/site-packages/torchvision/__init__.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/PGC/lib/python3.8/site-packages/torch/__init__.py:290\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m USE_GLOBAL_DEPS:\n\u001b[1;32m    289\u001b[0m         _load_global_deps()\n\u001b[0;32m--> 290\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_C\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;66;03m# Appease the type checker; ordinarily this binding is inserted by the\u001b[39;00m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;66;03m# torch._C module initialization code in C\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m TYPE_CHECKING:\n",
      "\u001b[0;31mImportError\u001b[0m: libnccl.so.2: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "\n",
    "os.environ['HF_ENDPOINT'] = \"https://hf-mirror.com\"\n",
    "\n",
    "class PETA_Dataset_Gender(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = {}\n",
    "        valid_extensions = {'.bmp', '.jpg', '.jpeg', '.png'}  # 支持的图片格式\n",
    "\n",
    "        # 遍历所有subset文件夹，读取标签文件\n",
    "        for subset in os.listdir(root_dir):\n",
    "            subset_path = os.path.join(root_dir, subset, 'archive')\n",
    "            if not os.path.isdir(subset_path):\n",
    "                continue\n",
    "\n",
    "            label_file = os.path.join(subset_path, 'Label.txt')\n",
    "\n",
    "            if not os.path.exists(label_file):\n",
    "                continue\n",
    "\n",
    "            # 读取标签文件，并保存每个ID的标签信息\n",
    "            with open(label_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    image_id = parts[0]\n",
    "                    tags = parts[1:]\n",
    "\n",
    "                    # 保存subset与ID组合后的唯一标识和personalMale标签的对应关系\n",
    "                    unique_id = f\"{subset}_{image_id}\"\n",
    "                    label = 1 if 'personalMale' in tags else 0\n",
    "                    self.labels[unique_id] = label\n",
    "\n",
    "            # 获取所有图片路径，支持多种图片格式\n",
    "            for img_file in os.listdir(subset_path):\n",
    "                ext = os.path.splitext(img_file)[-1].lower()\n",
    "                if ext in valid_extensions:\n",
    "                    # 获取文件名前的ID，即第一个下划线之前的部分\n",
    "                    image_id = img_file.split('_')[0]\n",
    "\n",
    "                    # 生成唯一ID，确保区分不同subset中的相同ID\n",
    "                    unique_id = f\"{subset}_{image_id}\"\n",
    "\n",
    "                    # 如果ID存在于标签文件中，则保存该图片路径和对应标签\n",
    "                    if unique_id in self.labels:\n",
    "                        img_path = os.path.join(subset_path, img_file)\n",
    "                        self.image_paths.append((img_path, self.labels[unique_id]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取图片路径和对应的标签\n",
    "        img_path, label = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)\n",
    "    \n",
    "\n",
    "class PETA_Dataset_Upper(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = []\n",
    "        self.labels = {}\n",
    "        valid_extensions = {'.bmp', '.jpg', '.jpeg', '.png'}  # 支持的图片格式\n",
    "        upperbody_colors = ['Black', 'Blue', 'Brown', 'Green', 'Grey', 'Orange', 'Pink', 'Purple', 'Red', 'White', 'Yellow']\n",
    "        color_to_label = {color: i for i, color in enumerate(upperbody_colors)}\n",
    "\n",
    "        # 遍历所有subset文件夹，读取标签文件\n",
    "        for subset in os.listdir(root_dir):\n",
    "            subset_path = os.path.join(root_dir, subset, 'archive')\n",
    "            if not os.path.isdir(subset_path):\n",
    "                continue\n",
    "\n",
    "            label_file = os.path.join(subset_path, 'Label.txt')\n",
    "\n",
    "            if not os.path.exists(label_file):\n",
    "                continue\n",
    "\n",
    "            # 读取标签文件，并保存每个ID的标签信息\n",
    "            with open(label_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    parts = line.strip().split()\n",
    "                    image_id = parts[0]\n",
    "                    tags = parts[1:]\n",
    "                    # print(tags)\n",
    "                    # input()\n",
    "                    # 默认标签为 -1，表示没有找到匹配的upperbody颜色\n",
    "                    label = -1\n",
    "\n",
    "                    # 查找upperbody颜色标签并转换为数字\n",
    "                    for color in upperbody_colors:\n",
    "                        color_tag = f'upperBody{color}'\n",
    "                        if color_tag in tags:\n",
    "                            label = color_to_label[color]\n",
    "                            break\n",
    "                        \n",
    "                    # 保存subset与ID组合后的唯一标识和personalMale标签的对应关系\n",
    "                    unique_id = f\"{subset}_{image_id}\"\n",
    "                    self.labels[unique_id] = label\n",
    "                    if label == -1:\n",
    "                        print(f\"Warning: No upperbody color found for {unique_id}\")\n",
    "\n",
    "            # 获取所有图片路径，支持多种图片格式\n",
    "            for img_file in os.listdir(subset_path):\n",
    "                ext = os.path.splitext(img_file)[-1].lower()\n",
    "                if ext in valid_extensions:\n",
    "                    # 获取文件名前的ID，即第一个下划线之前的部分\n",
    "                    image_id = img_file.split('_')[0]\n",
    "\n",
    "                    # 生成唯一ID，确保区分不同subset中的相同ID\n",
    "                    unique_id = f\"{subset}_{image_id}\"\n",
    "\n",
    "                    # 如果ID存在于标签文件中，则保存该图片路径和对应标签\n",
    "                    if unique_id in self.labels and self.labels[unique_id] != -1:\n",
    "                        img_path = os.path.join(subset_path, img_file)\n",
    "                        self.image_paths.append((img_path, self.labels[unique_id]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # 获取图片路径和对应的标签\n",
    "        img_path, label = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, torch.tensor(label, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No upperbody color found for 3DPeS_162\n",
      "Warning: No upperbody color found for CUHK_3688.png\n",
      "Warning: No upperbody color found for GRID_0180\n",
      "Warning: No upperbody color found for PRID_0298\n"
     ]
    }
   ],
   "source": [
    "Transform = transforms.Compose([transforms.Resize(size=(224, 224)), transforms.ToTensor()])\n",
    "# Full_Dataset = PETA_Dataset(root_dir='./PETA dataset', transform=Transform)\n",
    "Full_Dataset = PETA_Dataset_Upper(root_dir='./PETA dataset', transform=Transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Num_Lables = 2\n",
    "Num_Lables = 11\n",
    "Pre_Trained_Model = 'google/vit-base-patch16-224-in21k'\n",
    "ViT_Feature_Extractor = 'google/vit-base-patch16-224-in21k'\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 2e-5\n",
    "Num_Workers = 0\n",
    "SHUFFLE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples:  15192\n",
      "Number of test samples:  3798\n"
     ]
    }
   ],
   "source": [
    "import torch.utils.data as data\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "train_size = int(0.8 * len(Full_Dataset))\n",
    "test_size = len(Full_Dataset) - train_size\n",
    "\n",
    "\n",
    "train_ds, test_ds = torch.utils.data.random_split(Full_Dataset, [train_size, test_size])\n",
    "\n",
    "print(\"Number of train samples: \", len(train_ds))\n",
    "print(\"Number of test samples: \", len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = data.DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=SHUFFLE,  num_workers=Num_Workers, drop_last=True)\n",
    "test_loader  = data.DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=SHUFFLE, num_workers=Num_Workers, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import ViTModel\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import tqdm as notebook_tqdm\n",
    "\n",
    "class ViTForImageClassification(nn.Module):\n",
    "    def __init__(self, num_labels=Num_Lables):\n",
    "        super(ViTForImageClassification, self).__init__()\n",
    "        self.vit = ViTModel.from_pretrained(Pre_Trained_Model)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.classifier = nn.Linear(self.vit.config.hidden_size, num_labels)\n",
    "        self.num_labels = num_labels\n",
    "\n",
    "    def forward(self, pixel_values, labels):\n",
    "        outputs = self.vit(pixel_values=pixel_values)\n",
    "        output = self.dropout(outputs.last_hidden_state[:,0])\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "#         if labels is not None:\n",
    "#           loss_fct = nn.CrossEntropyLoss()\n",
    "#           loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        if loss is not None:\n",
    "          return logits, loss.item()\n",
    "        else:\n",
    "          return logits, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "from transformers import ViTFeatureExtractor\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lab/.local/lib/python3.8/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Define Model\n",
    "model = ViTForImageClassification(Num_Lables)\n",
    "# Feature Extractor\n",
    "feature_extractor = ViTFeatureExtractor.from_pretrained(ViT_Feature_Extractor, do_rescale=False)\n",
    "# Adam Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "# Cross Entropy Loss\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "# Use GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model= nn.DataParallel(model)\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5acd6f7796de4e7f80a142412bb43a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 7, 0, 0, 4, 4, 4, 0, 0, 4, 0, 0, 1, 2, 3, 0, 0, 2, 0, 9, 9, 9, 0, 9,\n",
      "        4, 0, 4, 4, 2, 9, 8, 4, 4, 9, 0, 0, 8, 0, 9, 4, 9, 0, 4, 4, 1, 0, 4, 8,\n",
      "        4, 0, 0, 0, 0, 9, 0, 7, 1, 0, 5, 2, 0, 2, 8, 0], device='cuda:0')\n",
      "Epoch:  0 | train loss: 0.0259 | test accuracy: 0.77\n",
      "tensor([0, 2, 0, 9, 0, 1, 4, 4, 9, 4, 0, 0, 9, 9, 4, 0, 4, 7, 0, 0, 9, 1, 0, 1,\n",
      "        1, 1, 0, 0, 9, 0, 3, 4, 1, 0, 0, 1, 4, 0, 2, 0, 4, 4, 0, 9, 0, 9, 9, 7,\n",
      "        4, 1, 0, 8, 4, 4, 8, 9, 4, 7, 8, 0, 1, 0, 4, 9], device='cuda:0')\n",
      "tensor([0, 0, 9, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 8, 9, 0, 9, 0, 4, 9, 4, 4, 0, 4,\n",
      "        0, 0, 0, 2, 9, 4, 0, 9, 8, 4, 0, 0, 0, 0, 0, 1, 4, 4, 1, 6, 0, 0, 9, 4,\n",
      "        0, 4, 9, 9, 2, 0, 0, 1, 9, 0, 4, 4, 9, 9, 4, 4], device='cuda:0')\n",
      "tensor([ 0,  0,  0,  7,  9,  9,  1,  0,  4,  0,  9,  0,  1,  0,  4,  8,  0,  0,\n",
      "         0,  9,  0,  4,  2,  0,  2,  0,  4,  4,  9,  3,  9,  0,  4,  4,  2,  0,\n",
      "         0,  1,  0,  2,  8,  4,  7,  0,  7,  0,  4,  0,  7,  4,  3,  0,  9, 10,\n",
      "         7,  0,  0,  1,  4,  0,  0,  8,  9,  0], device='cuda:0')\n",
      "tensor([1, 0, 0, 9, 0, 0, 3, 0, 0, 1, 8, 9, 0, 0, 9, 1, 0, 9, 8, 0, 4, 3, 1, 1,\n",
      "        0, 0, 2, 4, 0, 0, 0, 4, 9, 9, 4, 0, 0, 4, 4, 1, 0, 0, 1, 4, 0, 1, 0, 0,\n",
      "        9, 0, 9, 9, 0, 1, 0, 0, 0, 3, 2, 3, 0, 0, 0, 0], device='cuda:0')\n",
      "tensor([9, 0, 1, 0, 0, 4, 0, 4, 0, 0, 9, 5, 0, 9, 9, 8, 0, 4, 1, 2, 0, 0, 9, 2,\n",
      "        0, 9, 0, 0, 1, 4, 1, 4, 0, 0, 0, 9, 0, 0, 4, 9, 9, 7, 0, 4, 9, 4, 0, 0,\n",
      "        0, 7, 8, 0, 0, 0, 0, 0, 8, 0, 4, 0, 4, 4, 4, 9], device='cuda:0')\n",
      "tensor([0, 4, 9, 4, 3, 0, 1, 0, 0, 0, 0, 3, 0, 9, 0, 4, 0, 0, 0, 3, 0, 4, 0, 0,\n",
      "        3, 0, 0, 0, 4, 1, 4, 4, 0, 7, 0, 0, 0, 6, 0, 9, 0, 0, 0, 8, 7, 4, 9, 0,\n",
      "        0, 1, 3, 9, 0, 9, 1, 1, 0, 0, 0, 4, 0, 0, 9, 0], device='cuda:0')\n",
      "tensor([4, 2, 4, 4, 4, 2, 4, 0, 9, 1, 0, 9, 0, 0, 9, 0, 0, 4, 9, 0, 1, 0, 9, 4,\n",
      "        0, 4, 0, 9, 0, 7, 0, 0, 0, 0, 0, 0, 0, 9, 4, 9, 9, 8, 0, 0, 2, 2, 8, 0,\n",
      "        0, 0, 7, 2, 0, 4, 1, 0, 0, 4, 0, 7, 0, 9, 4, 0], device='cuda:0')\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Feed through model\u001b[39;00m\n\u001b[1;32m     23\u001b[0m output, loss_train \u001b[38;5;241m=\u001b[39m model(b_x, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;43mprint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Calculate loss\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss_train \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:463\u001b[0m, in \u001b[0;36mTensor.__repr__\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    460\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__repr__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, tensor_contents\u001b[38;5;241m=\u001b[39mtensor_contents\n\u001b[1;32m    461\u001b[0m     )\n\u001b[1;32m    462\u001b[0m \u001b[38;5;66;03m# All strings are unicode in Python 3.\u001b[39;00m\n\u001b[0;32m--> 463\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor_str\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor_str.py:698\u001b[0m, in \u001b[0;36m_str\u001b[0;34m(self, tensor_contents)\u001b[0m\n\u001b[1;32m    696\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(), torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39m_python_dispatch\u001b[38;5;241m.\u001b[39m_disable_current_modes():\n\u001b[1;32m    697\u001b[0m     guard \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_DisableFuncTorch()\n\u001b[0;32m--> 698\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_str_intern\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_contents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_contents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor_str.py:618\u001b[0m, in \u001b[0;36m_str_intern\u001b[0;34m(inp, tensor_contents)\u001b[0m\n\u001b[1;32m    616\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m _tensor_str(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_dense(), indent)\n\u001b[1;32m    617\u001b[0m                 \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 618\u001b[0m                     tensor_str \u001b[38;5;241m=\u001b[39m \u001b[43m_tensor_str\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout \u001b[38;5;241m!=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstrided:\n\u001b[1;32m    621\u001b[0m     suffixes\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlayout=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayout))\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor_str.py:350\u001b[0m, in \u001b[0;36m_tensor_str\u001b[0;34m(self, indent)\u001b[0m\n\u001b[1;32m    346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\n\u001b[1;32m    347\u001b[0m         \u001b[38;5;28mself\u001b[39m, indent, summarize, real_formatter, imag_formatter\n\u001b[1;32m    348\u001b[0m     )\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 350\u001b[0m     formatter \u001b[38;5;241m=\u001b[39m \u001b[43m_Formatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_summarized_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msummarize\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _tensor_str_with_formatter(\u001b[38;5;28mself\u001b[39m, indent, summarize, formatter)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor_str.py:134\u001b[0m, in \u001b[0;36m_Formatter.__init__\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfloating_dtype:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m value \u001b[38;5;129;01min\u001b[39;00m tensor_view:\n\u001b[0;32m--> 134\u001b[0m         value_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_width, \u001b[38;5;28mlen\u001b[39m(value_str))\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:986\u001b[0m, in \u001b[0;36mTensor.__format__\u001b[0;34m(self, format_spec)\u001b[0m\n\u001b[1;32m    984\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m, format_spec)\n\u001b[1;32m    985\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_meta \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m Tensor:\n\u001b[0;32m--> 986\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(format_spec)\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__format__\u001b[39m(\u001b[38;5;28mself\u001b[39m, format_spec)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "# Train the model\n",
    "acc = []\n",
    "lss = []\n",
    "ep = []\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for step, (x, y) in (p:=tqdm(enumerate(train_loader))):\n",
    "        p.set_description(f'Epoch {epoch}: {step}/{len(train_loader)}')\n",
    "        # Change input array into list with each batch being one element\n",
    "        x = np.array_split(np.squeeze(np.array(x)), BATCH_SIZE)\n",
    "        # Remove unecessary dimension\n",
    "        for index, array in enumerate(x):\n",
    "            x[index] = np.squeeze(array)\n",
    "        # Apply feature extractor, stack back into 1 tensor and then convert to tensor\n",
    "\n",
    "        x = torch.tensor(np.stack(feature_extractor(x)['pixel_values'], axis=0))\n",
    "        # Send to GPU if available\n",
    "        x, y  = x.to(device), y.to(device)\n",
    "        b_x = Variable(x)   # batch x (image)\n",
    "        b_y = Variable(y)   # batch y (target)\n",
    "        # Feed through model\n",
    "        output, loss_train = model(b_x, None)\n",
    "        print(b_y)\n",
    "        # Calculate loss\n",
    "        if loss_train is None:\n",
    "            loss_train = loss_func(output, b_y)\n",
    "            optimizer.zero_grad()\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "          # train_losses.append(loss_train.item())\n",
    "        if step % 50 == 0:\n",
    "            # Get the next batch for testing purposes\n",
    "            test = next(iter(test_loader))\n",
    "            test_x = test[0]\n",
    "            # Reshape and get feature matrices as needed\n",
    "\n",
    "            test_x = np.array_split(np.squeeze(np.array(test_x)), BATCH_SIZE)\n",
    "            for index, array in enumerate(test_x):\n",
    "                test_x[index] = np.squeeze(array)\n",
    "            test_x = torch.tensor(np.stack(feature_extractor(test_x)['pixel_values'], axis=0))\n",
    "            # Send to appropirate computing device\n",
    "            test_x = test_x.to(device)\n",
    "            test_y = test[1].to(device)\n",
    "            # Get output (+ respective class) and compare to target\n",
    "            test_output, loss_test = model(test_x, test_y)\n",
    "            # val_losses.append(loss_test.item())\n",
    "            test_output = test_output.argmax(1)\n",
    "            # Calculate Accuracy\n",
    "            accuracy = (test_output == test_y).sum().item() / BATCH_SIZE\n",
    "            # accuracy_all.append(accuracy)\n",
    "            print('Epoch: ', epoch, '| train loss: %.4f' % loss_train, '| test accuracy: %.2f' % accuracy)\n",
    "            acc.append(accuracy)\n",
    "            ep.append(step)\n",
    "            step = step + 1\n",
    "            lss.append(loss_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "torch.save(model, 'model-upper.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m transforms\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/PGC/lib/python3.8/site-packages/torchvision/__init__.py:5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodulefinder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Module\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Don't re-order these, we need to load the _C extension (done when importing\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# .extensions) before entering _meta_registrations.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mextension\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _HAS_OPS  \u001b[38;5;66;03m# usort:skip\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "\n",
    "\n",
    "MODEL_PATH = 'model-upper.pt'\n",
    "model = torch.load(MODEL_PATH)\n",
    "model.eval()\n",
    "\n",
    "\n",
    "eval_loader  = data.DataLoader(test_ds, batch_size=1, shuffle=SHUFFLE, num_workers=Num_Workers, drop_last=True)\n",
    "\n",
    "print(len(eval_loader), device)\n",
    "# break\n",
    "# Disable grad\n",
    "with torch.no_grad():\n",
    "  # for step, (inputs, target) in enumerate(train_loader):\n",
    "  for i in range(len(eval_loader)):\n",
    "  #  for i in range(10):\n",
    "    inputs, target = next(iter(eval_loader))\n",
    "    # Reshape and get feature matrices as needed\n",
    "    # print(inputs.shape)\n",
    "    # break\n",
    "    inputs = inputs[0].permute(1, 2, 0)\n",
    "    # Save original Input\n",
    "    originalInput = inputs\n",
    "    for index, array in enumerate(inputs):\n",
    "      inputs[index] = np.squeeze(array)\n",
    "    inputs = torch.tensor(np.stack(feature_extractor(inputs)['pixel_values'], axis=0))\n",
    "\n",
    "    # Send to appropriate computing device\n",
    "    inputs = inputs.to(device)\n",
    "    target = target.to(device)\n",
    "    # print(inputs.shape, target.shape)\n",
    "    # Generate prediction\n",
    "    prediction, loss = model(inputs, target)\n",
    "    # break\n",
    "    # Predicted class value using argmax\n",
    "    predicted_class = np.argmax(prediction.cpu())\n",
    "    print(predicted_class, target)\n",
    "    # break\n",
    "    # value_predicted = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(predicted_class)]\n",
    "    # value_target = list(valid_ds.class_to_idx.keys())[list(valid_ds.class_to_idx.values()).index(target)]\n",
    "    # print(predicted_class, target)\n",
    "    # value_predicted = list(valid_ds.class_to_idx.values())[list(valid_ds.class_to_idx.values()).index(predicted_class)]\n",
    "    # value_target = list(valid_ds.class_to_idx.values())[list(valid_ds.class_to_idx.values()).index(target)]\n",
    "\n",
    "    # Actual_Labels.append(value_target)\n",
    "    # Predicted_Labels.append(value_predicted)\n",
    "\n",
    "    # # print(value_predicted==value_target)\n",
    "    # print(f'Image - {i}, Prediction: {value_predicted} - Actual target: {value_target}; {value_predicted==value_target}')\n",
    "    # Show result\n",
    "    # plt.imshow(originalInput)\n",
    "    # plt.xlim(224,0)\n",
    "    # plt.ylim(224,0)\n",
    "    # plt.title(f'Prediction: {value_predicted} - Actual target: {value_target}')\n",
    "    # plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PGC",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
